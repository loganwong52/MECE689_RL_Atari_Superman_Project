{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f63db7d2-9b4f-485c-bd44-000f4c706816",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4893689e-cee7-4973-aabe-39a77e6ab014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7416746a-e790-42ae-ab27-8e04ecd94aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment again\n",
    "env = gym.make(\"ALE/Superman-v5\", render_mode=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a321e2d-4506-4ef2-9369-4c7008118342",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Logan\\anaconda3\\envs\\superman_env\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:78: UserWarning: The `render_mode` attribute is not defined in your environment. It will be set to None.\n",
      "  warnings.warn(\"The `render_mode` attribute is not defined in your environment. It will be set to None.\")\n",
      "C:\\Users\\Logan\\anaconda3\\envs\\superman_env\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:242: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 4.03GB > 3.65GB\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a NEW DQN model with the same architecture\n",
    "model = DQN(\"CnnPolicy\", env, buffer_size=20000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db1a8115-fca9-499a-a260-65cd037dc6dd",
   "metadata": {},
   "source": [
    "## Load the saved policy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43158eeb-e94d-4429-8b69-b38219fbbbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained weights directly into the policy network\n",
    "model.policy = model.policy.load(\"dqn_superman_policy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81210c83-fab7-43c6-95ad-ce9b43a32c3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abc0180-8087-4938-a4d9-cea71c8271be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bfc8341d-9ec1-4f10-884e-ed2da2424b38",
   "metadata": {},
   "source": [
    "## Evaluate Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dcbabab-b00a-4bf5-8e06-86d0f8ab616b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model over 10 episodes\n",
    "# Run the agent through 10 complete games & calculate the average score. \n",
    "# deterministic=True means it to always choose the best action it knows (instead of exploring randomly)\n",
    "\n",
    "# mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=10, deterministic=True)\n",
    "# print(\"Done evaluating!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff4db870-593b-4209-a841-8b0d9f61e12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running evaluation of trained DQN agent...\n",
      "Episode 1: Reward =    0.0, Steps = 5000\n",
      "Episode 2: Reward =    0.0, Steps = 5000\n",
      "Episode 3: Reward =    0.0, Steps = 5000\n",
      "Episode 4: Reward =    0.0, Steps = 5000\n",
      "Episode 5: Reward =    0.0, Steps = 5000\n",
      "Episode 6: Reward =    0.0, Steps = 5000\n",
      "Episode 7: Reward =    0.0, Steps = 5000\n",
      "Episode 8: Reward =    0.0, Steps = 5000\n",
      "Episode 9: Reward =    0.0, Steps = 5000\n",
      "Episode 10: Reward =    0.0, Steps = 5000\n"
     ]
    }
   ],
   "source": [
    "all_rewards = []\n",
    "all_lengths = []\n",
    "\n",
    "print(\"Running evaluation of trained DQN agent\")\n",
    "for episode in range(10):\n",
    "    obs, _ = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    \n",
    "    while not done and steps < 5000:  # Add a step limit to prevent infinite episodes\n",
    "        action, _ = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        steps += 1\n",
    "        done = done or truncated\n",
    "    \n",
    "    all_rewards.append(total_reward)\n",
    "    all_lengths.append(steps)\n",
    "    print(f\"Episode {episode+1}: Reward = {total_reward:6.1f}, Steps = {steps}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7352e5fb-f5ae-4c38-b750-b66105f29977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DQN PERFORMANCE REPORT:\n",
      "Mean reward: 0.00\n",
      "Median reward: 0.00\n",
      "Min reward: 0.00\n",
      "Max reward: 0.00\n",
      "Standard deviation: 0.00\n",
      "Success rate (reward > 0): 0.0%\n",
      "Average episode length: 5000.0 steps\n"
     ]
    }
   ],
   "source": [
    "# Calculate comprehensive statistics\n",
    "rewards_array = np.array(all_rewards)\n",
    "lengths_array = np.array(all_lengths)\n",
    "\n",
    "print(\"DQN PERFORMANCE REPORT:\")\n",
    "print(f\"Mean reward: {np.mean(rewards_array):.2f}\")\n",
    "print(f\"Median reward: {np.median(rewards_array):.2f}\")\n",
    "print(f\"Min reward: {np.min(rewards_array):.2f}\") \n",
    "print(f\"Max reward: {np.max(rewards_array):.2f}\")\n",
    "print(f\"Standard deviation: {np.std(rewards_array):.2f}\")\n",
    "print(f\"Success rate (reward > 0): {np.mean(rewards_array > 0) * 100:.1f}%\")\n",
    "print(f\"Average episode length: {np.mean(lengths_array):.1f} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eb758c-b791-4521-9ebe-ad49b18054e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d093d5-2527-468f-baf6-ce509b39103b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83be3fe-1c38-4f7d-baf8-fcd0efe9894d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9171e848-d08b-4f34-8f91-fa7c2ac23276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"Mean reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "# If mean_reward > 0: agent has learned something!\n",
    "# This is bc agent usually gets 0 reward in Superman\n",
    "# This will be the baseline performance..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7535ee-6d20-4c07-8eb0-176c9ba53b5c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fef2725-b9c1-4459-a788-f5326615a780",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8716e0ce-809c-4f30-b334-8340277f39d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcfc5c2-d877-4412-9e23-69cb31df2054",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b12bca6a-ca45-4590-b898-047efbfe11fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee56227-4f8d-4aef-a6e4-ac58b94cb4c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a72c6a-742c-4937-ad29-ef2cc63f978b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5bb9ecb-1f10-4d57-afda-3b2b33911bc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbf1329-42ea-48b6-b526-9b5fe68fabd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed1f12a-b80e-4ebd-b771-ecb00af3f2d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea07eabf-2be8-48c3-9873-0e539579f4e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
